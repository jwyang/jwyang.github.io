---
permalink: /
title: "Welcome to My Homepage!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I am a senior researcher in [Deep Learning Group](https://www.microsoft.com/en-us/research/group/deep-learning-group/) at Microsoft Research, Redmond, directed by [Dr. Jianfeng Gao](http://research.microsoft.com/en-us/um/people/jfgao/). My research interests span in computer vision, vision & language and machine learning. More specifically, my primary researches are about structured visual understanding at different levels and how to further leverage them for intelligent interactions with human through language and environment through embodiment. I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agent.

Prior to joining Microsoft at March 2020, I earned my Ph.D. in Computer Science from School of Interactive Computing at [Georgia Tech](https://www.gatech.edu) with thesis "Structured Visual Understanding, Generation and Reasoning". I was fortunte to be supervised by [Prof. Devi Parikh](https://cc.gatech.edu/~parikh/) and work closely with [Prof. Dhruv Batra](https://www.cc.gatech.edu/~dbatra/).

**If you are interested in working with me as a research intern, please feel free to drop me an email through jianwei.yang at microsoft dot com.**

<h2><img src="/images/fire.png" width="3%"/> <span style="color:red; font-family:Papyrus">News</span></h2>
  <img src="/images/dart.png" width="2.5%"/> \[06/2022\] We are releasing the [code](https://github.com/microsoft/RegionCLIP) for our CVPR 2022 paper [RegionCLIP](https://arxiv.org/abs/2112.09106), and also a [live demo](https://huggingface.co/spaces/CVPR/regionclip-demo) on huggingface!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[04/2022\] We are releasing our CVPR 2022 paper UniCL [paper](https://arxiv.org/abs/2204.03610) [code](https://github.com/microsoft/UniCL) [demo](https://huggingface.co/spaces/CVPR/unicl-zero-shot-img-recog) - a unified contrastive learning paradigm to learn discriminative and semantic-rich representations from image-label AND image-text data seamlessly!<br/>
<img src="/images/dart.png" width="2.5%"/> \[03/2022\] We are releasing FocalNet [paper](https://arxiv.org/abs/2203.11926) [code](https://github.com/microsoft/FocalNet) - a simple, effective and attention-free architecture for vision!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[03/2022\] Three papers got accepted by CVPR 2022, see you in New Orleans!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[09/2021\] Gave a talk on [Vision Transformers for supervised/self-supervised Learning](https://youtu.be/fk-6JdRjLPw)<br/>
<img src="/images/dart.png" width="2.5%"/> \[09/2021\] Our Focal Transformer is accepted to NeurIPS 2021 as Spotlight!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[08/2021\] We released code for Focal Transformers [here](https://github.com/microsoft/Focal-Transformer)!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[07/2021\] Four papers are accepted to ICCV 2021! Congratulations to all authors!<br/>
<img src="/images/dart.png" width="2.5%"/> \[07/2021\] We are releasing [Focal Transformer](https://arxiv.org/pdf/2107.00641.pdf), achieving new SoTA on COCO object detection, instance segmentation and ADE20K semantic segmentation!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[06/2021\] We are releasing [EsViT](https://arxiv.org/pdf/2106.09785.pdf), a much more efficient self-supervised learning pipeline reaching new SoTA on ImageNet-1k linear probe!<br/>
