---
permalink: /
title: "Welcome to My Homepage!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I am a senior researcher in [Deep Learning Group](https://www.microsoft.com/en-us/research/group/deep-learning-group/) at Microsoft Research, Redmond, directed by [Dr. Jianfeng Gao](http://research.microsoft.com/en-us/um/people/jfgao/). My research interests span in computer vision, vision & language and machine learning. More specifically, my primary researches are about structured visual understanding at different levels and how to further leverage them for intelligent interactions with human through language and environment through embodiment. I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agent.

Prior to joining Microsoft at March 2020, I earned my Ph.D. in Computer Science from School of Interactive Computing at [Georgia Tech](https://www.gatech.edu) with thesis "Structured Visual Understanding, Generation and Reasoning". I was honored to be supervised by [Prof. Devi Parikh](https://cc.gatech.edu/~parikh/) and work closely with [Prof. Dhruv Batra](https://www.cc.gatech.edu/~dbatra/). Here is my old [GT homepage](https://www.cc.gatech.edu/~jyang375/).

**If you are interested in working with me as a research intern, please feel free to drop me an email through jianwei.yang at microsoft dot com.**

<h2><img src="/images/fire.png" width="3%"/> <span style="color:red; font-family:Papyrus">News</span></h2>
  <img src="/images/dart.png" width="2.5%"/> \[07/2021\] We are releasing Focal Transformer [arXiv paper](https://arxiv.org/pdf/2107.00641.pdf). Using the proposed focal self-attention, we achieve new SoTA on COCO object detection, instance segmentation and ADE20K semantic segmentation!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[06/2021\] We are releasing EsViT [arXiv paper](https://arxiv.org/pdf/2106.09785.pdf), a much more efficient self-supervised learning pipeline reaching new SoTA on ImageNet-1k linear probe, check it out!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[04/2021\] We are introducing Vision Longformer [arXiv paper](https://arxiv.org/pdf/2103.15358.pdf) which is an advanced version of vision transformer that achieves significant improvements on image classification, object detection comparing with ResNet and PVT!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[01/2021\] We show in our [arXiv paper](https://arxiv.org/pdf/2101.00529.pdf) that vision feature matters significantly for vision-language tasks!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[12/2020\] We release our [arXiv paper](https://arxiv.org/pdf/2012.11587.pdf) studying the visual reasoning capacity in visual question answering models!<br/>
  <img src="/images/dart.png" width="2.5%"/> \[11/2020\] We release our [arXiv paper](https://arxiv.org/pdf/2011.09530.pdf) leveraging token relationships to learn neural-symbolic video captioning!<br/>
